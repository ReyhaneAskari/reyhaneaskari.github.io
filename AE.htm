<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109957767-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-109957767-1');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      extensions: ["tex2jax.js"],
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
      tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
      TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
      messageStyle: "none"
    });
    </script>    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>

  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    img {
    display: block;
    margin: 0 auto;
   }
  </style>
  <meta http-equiv="Content-Type" content="text.php; charset=UTF-8">
      <meta charset="utf-8">
      <link rel="stylesheet" href="./css/bootstrap.min.css">
      <script src="./css/jquery.min.js"></script><style type="text/css"></style>
      <script src="./css/bootstrap.min.js"></script>
      <link rel="stylesheet" type="text/css" href="./css/style.css">
      <link href="./css/font-awesome.min.css" rel="stylesheet">
      <link href="./css/css" rel="stylesheet" type="text/css">


  <div class="navbar navbar-default ">
      <div class="container">
          <div class="navbar-header">
              <a class="navbar-brand" href="./index.htm">

               <strong>Reyhane Askari</strong>
              </a>
          </div>
          <div class="navbar-collapse collapse">
              <ul class="nav navbar-nav navbar-right">
                  <!-- <li><a href="./index.htm">About</a></li> -->
              </ul>
          </div>
      </div>
  </div>
  </head>
  <body>
    <title>Auto Encoders</title>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody>
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Auto Encoders</name>
        </p>
        <p>
          Auto encoders are one of the unsupervised deep learning models. The aim of an auto encoder is 
          dimensionality reduction and feature discovery. An auto encoder is trained to predict its own input, but to
          prevent the model from learning the identity mapping, some constraints are applied to the hidden units.
        </p>
        <p>
          The simplest form of an auto encoder is a feedforward neural network where the input <mathjax>$x$</mathjax> is fed to the hidden layer of <mathjax>$h(x)$</mathjax> and <mathjax>$h(x)$</mathjax> is then fed to calculate the output <mathjax>$\hat{x}$</mathjax>. A simple auto encoder is shown in Fig1.
        </p>
        <figure>
          <img src="./Reyhane Askari_files/ae.png">
          <figcaption align="center">Fig1. - A simple Autoencoder.</figcaption>
        </figure>
        <br>
        <p></p> The following equation can describe an autoencoder:
        <p><mathjax>$$\hat{x} = O(a(h) ) = Sigmoid ( c + w^{*} h(x)),$$</mathjax></p>
        <p><mathjax>$$h(x) = g(a(x)) = Sigmoid (b + Wx),$$</mathjax></p>
        <p>where <mathjax>$a$</mathjax> is a linear transformation and both <mathjax>$O, g$</mathjax> are activation functions.</p>
        <p> The autoencoder tries to reconstruct the input. So if inputs are real values, the loss function can be computed as the following mean square error (MSE):
        </p>
        <p><mathjax>$$l =  \frac{1}{2} \sum_{k=1}^N (x_{k} - \hat{x_{k}})^2,$$</mathjax></p>
        <p>where <mathjax>$N$</mathjax> is the number of examples. But if the inputs are binary, we can define our loss function as a binary cross entropy between each pixel of the target (which is the input itself) and the output. In this case the output can be considered as a probability:</p>
        <p><mathjax>$$l =  - \sum_{k=1}^N \big\{x_{k} log (\hat{x_{k}}) + (1 - x_{k}) log(1 - \hat{x_{k}})\big\}$$</mathjax></p>
        <p>It can be shown that if a single layer linear autoencoder with no activation function is used, the subspace spanned by AE's weights is the same as PCA's subspace.</p>
        <h2 align="center">PyTorch Experiments <a href = "https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/AE.py">(Github link)</a></h2>
        <p><a href = "https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/AE.py">Here</a> is a link to a simple Autoencoder in PyTorch. MNIST is used as the dataset. The input is binarized and Binary Cross Entropy has been used as the loss function. The hidden layer contains 64 units. The Fig. 2 shows the reconstructions at 1st, 100th and 200th epochs:
        </p>
        <figure>
          <img src="./Reyhane Askari_files/ae_out.png">
          <figcaption align="center">Fig. 2 - Reconstructions by an Autoencoder. From left to right: 1st, 100th and 200th epochs.</figcaption>
        </figure>
      </tr>
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Denoising Auto Encoders (DAE)</name>
        </p>
        <p>
          In a denoising auto encoder the goal is to create a more robust model to noise. The motivation is that the hidden layer should be able to capture high level representations and be robust to small changes in the input. The input of a DAE is noisy data but the target is the original data without noise:
        </p>
        <p><mathjax>$$\hat{\tilde{x}} = O(a(h) ) = Sigmoid ( c + w^{*} h(\tilde{x})),$$</mathjax></p>
        <p><mathjax>$$h(\tilde{x}) = g(a(\tilde{x})) = Sigmoid (b + W\tilde{x}),$$</mathjax></p>
        <p><mathjax>$$l = \frac{1}{2} \sum_{k=1}^N (x_{k} - \hat{\tilde{x_{k}}})^2$$</mathjax></p>
        <p><mathjax>$$l =  - \sum_{k=1}^N \big\{x_{k} log (\hat{\tilde{x}}_{k}) + (1 - x_{k}) log(1 - \hat{\tilde{x_{k}}})\big\}$$</mathjax></p>
        <p>
          Where <mathjax>$\tilde{x}$</mathjax> is the noisy input. DAE can be used to denoise the input.
        </p>
        <h2 align="center">PyTorch Experiments <a href = "https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/DAE.py">(Github link)</a></h2>
        <p>
          <a href = "https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/DAE.py">Here</a> is a PyTorch implementation of a DAE. To train a DAE, we can simply add some random noise to the data and create corrupted inputs. In this case 20% noise has been added to the input. The Fig3 shows the input (<mathjax>$x$</mathjax>), noisy input (<mathjax>$\tilde{x}$</mathjax>) and the reconstructed samples (<mathjax>$\hat{\tilde{x}}$</mathjax>) in the 200th epoch.
        </p>
        <figure>
          <img src="./Reyhane Askari_files/dae_out.png">
          <figcaption align="center">Fig3 - Denoising Autoencoders: From left to right: input, noisy input and output of the 200th epoch.</figcaption>
        </figure>
        <figure>
          <img src="./Reyhane Askari_files/filters.png">
          <figcaption align="center">Fig4. DAE filters, the filters in a DAE show more distinctive features.</figcaption>
        </figure>
        </td>
      </tr>
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Variational Auto Encoders (VAE)</name>
        </p>
        <p>
          In a VAE, there is a strong assumption for the distribution that is learned in the hidden representation. The hidden representation is constrained to be a multivariate guassian. The motivation behind this is that we assume the hidden representation learns high level features and these features follow a very simple form of distribiution. Thus, we assume that each feature is a guassian distribiution and their combination which creates the hidden representation is a multivariate guassian. 
        </p>
        <p> From a probabilistic graphical models prespective, an auto encoder can be seen as a directed graphical model where the hidden units are latent variables (<mathjax>$z$</mathjax>) and the following rule applies:
        </p>
        <p>
          <mathjax>$$p_{\theta}(x,\ z) = p_{\theta}(z)\ p_{\theta}(x|z),$$</mathjax>
        </p>
        <p>where <mathjax>$\theta$</mathjax> indicates that <mathjax>$p$</mathjax> is parametrized by <mathjax>$\theta$</mathjax>. And according to the Bayes rule, the likelihood of the data (<mathjax>$p_{\theta}(x)$</mathjax>) can be derived using the following:
        </p>
        <mathjax>$$p_{\theta}(x) =  \frac{p_{\theta}(x|\ z)\ p_{\theta}(z)}{p_{\theta}(z,\ x)},$$</mathjax>        
        <p> <mathjax>$p_{\theta}(x|z)$</mathjax> is the distribiution that generates the data (<mathjax>$x$</mathjax>) and is tractable using the dataset. In a VAE it is assumed the prior distribiution (<mathjax>$p_{\theta}(z)$</mathjax>) is a multivariate normal distribiution (centered at zero with co-varience of <mathjax>$I$</mathjax>):
        </p>
        <p>
          <mathjax>$$p_{\theta}(z) = \prod_{k=1} ^ N  \mathcal{N}(z_{k}\ |\ 0,1)$$</mathjax>
        </p>
        <p>
          The posterior distribiution (<mathjax>$p_{\theta}(z|x)$</mathjax>) is an intractable distribiution (never observed), but the encoder learns <mathjax>$q_{\varphi}(z|x)$</mathjax> as its estimator. As mentioned above, we assume <mathjax>$q_{\varphi}(z|x)$</mathjax> is a normal distribiution which is parameterized by <mathjax>${\varphi}$</mathjax>:
        </p>
        <p>
          <mathjax>$$q_{\varphi}(z|x) = \prod_{k=1} ^ N  \mathcal{N}(z_{k}\ |\ \mu_{k}(x),\ \sigma_{k}^2(x)).$$</mathjax>
        </p>
        <p>
          Now the likelihood is parameterized by <mathjax>$\theta$</mathjax> and <mathjax>$\varphi$</mathjax>. The goal is to find a <mathjax>$\theta^{*}$</mathjax> and a <mathjax>$\varphi^{*}$</mathjax> such that <mathjax>$log\ p_{\theta, \varphi}(x)$</mathjax> is maximized. Or equivallently, we minimize the negative log-likelihood (nll):
        </p>
        <p>In this setting, the following is a lower-bound on the log-likelihood of <mathjax>$x$</mathjax>. <a href="https://arxiv.org/abs/1312.6114">(Kingma and Welling, 2014.):</a> </p>
        <p><mathjax>$$\mathcal{L}(x) = - D_{kl}\ (q_{\varphi}(z|x)\ ||\ p_{\theta}(z)) + E_{q_{\varphi}\ (z|x)}[\ log p_{\theta}(x\ |\ z)],$$</mathjax>
        </p>
        <p>The second term is a reconstruction error which is approximated by sampling from <mathjax>$q_{\varphi}(z|x)$</mathjax> (the encoder) and then computing <mathjax>$p_{\theta}(x\ |\ z)$</mathjax> (the decoder). The first term, <mathjax>$D_{kl}$</mathjax> is the Kullback–Leibler divergence which measures the differnce between two probability distribiutions. The KL term encourges the model to learn a <mathjax>$q_{\varphi}(z|x)$</mathjax> that is of the form of <mathjax>$p_{\theta}(z)$</mathjax> which is a normal distribiution and acts as a regularizer. Considering that <mathjax>$p_{\theta}(z)$</mathjax> and <mathjax>$q_{\varphi}(z|x)$</mathjax> are normal distribiutions, the KL term can be simplified to the following form:
        </p>
        <mathjax>$$ D_{kl}= \frac{1}{2}\ \sum_{k = 1}^{N}\ 1\ +\ log(\sigma_{k}^2 (x))- \mu_{k}^{2} (x) - \sigma_{k}^2 (x)$$</mathjax>
        <figure>
          <img src="./Reyhane Askari_files/vae.png">
          <figcaption align="center">Fig5. Variational Auto Encoder: the hidden units parameterize the multivariate normal distribiution <mathjax>$q_{\varphi}(z|x)$</mathjax> by finding a set of <mathjax>$(\ \mu,\ \sigma^2)$</mathjax>. <mathjax>$ \mathcal{N}$</mathjax> is a normal distribiution from which we sample and is used in the reparametrization trick.</figcaption>
        </figure>
        <h3 align="center">In short</h3>
        <p>A variational autoencoder has a very similar structure to an autoencoder except for several changes:
        <ul>
          <li>Strong assumption that the hidden representation follows a guassian distribiution.</li>
          <li>The loss function has a new regularizer term (KL term) which forces the hidden representation to be close to a normal distribiution.</li>
          <li>The model can be used for generation. Since the KL term makes sure that <mathjax>$q_{\varphi}(z|x)$</mathjax> and <mathjax>$p_{\theta}(z)$</mathjax> are close, one can sample from <mathjax>$q_{\varphi}(z|x)$</mathjax> to generate new datapoints which will look very much like training samples.</li>
        </ul>
      </p>
        <h3 align="center">The Reparametrization Trick</h3>
        <p>
          The problem that might come to ones mind is that how the gradient flows through a VAE where it involves sampling from <mathjax>$q_{\varphi}(z|x)$</mathjax> which is a non-deterministic procedure. To tackle this problem, the reparametrization trick is used. In order to have a sample from the distribiution <mathjax>$ \mathcal{N}(\ \mu,\ \sigma^2)$</mathjax>, one can first sample from a normal distribiution <mathjax>$ \mathcal{N}(\ 0,\ 1)$</mathjax> and then calculate:
        </p>
        <p><mathjax>$$ \mathcal{N}(\ \mu,\ \sigma^2) = \mathcal{N}(\ 0,\ 1) * \sigma^2 + \mu$$</mathjax></p>
        <figure>
          <img src="./Reyhane Askari_files/reparam.png">
          <figcaption align="center">Fig6. The reparametrization Trick: sampling from <mathjax>$ \mathcal{N}(\ \mu,\ \sigma^2)$</mathjax> is non-deteministic and thus the gradient can not be passed through it. If one samples from the noraml distribiution <mathjax>$ \mathcal{N}(\ 0,\ 1)$</mathjax> and just scale and move it using the parameters <mathjax>$\sigma^2,\ \mu$</mathjax>, the backward pass will be deterministic.</figcaption>
        </figure>
        <h2 align="center">PyTorch Experiments <a href = "https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/VAE.py">(Github link)</a></h2>
        <p><a href = "https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/VAE.py">Here</a> is a PyTorch implementation of a VAE. The code also generates new samples. It also does a generation with interpolation which means that it starts from one sample and moves towards another sample in the latent space and generates new samples by interpolating between the mean and the variance of the first and second sample. Fig7. shows the generated samples with interpolation.</p>
        <figure>
          <img src="./Reyhane Askari_files/vae_generated.png">
          <figcaption align="center">Fig7. From left to right: generated samples with interpolation at 1st, 40th, 80th, 150th and 200th epochs</figcaption>
        </figure>
      </tr>
      </tbody></table>
      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script><script src="./Reyhane Askari_files/ga.js" type="text/javascript"></script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </tbody></table>
  

</body></html>
